{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd9657d",
   "metadata": {},
   "source": [
    "Banking Fraud Detection System with Generative AI\n",
    "\n",
    "## Complete End-to-End Solution\n",
    "\n",
    "This notebook contains:\n",
    "- **Embedding-Based Anomaly Detection** (4 ML models)\n",
    "- **Natural Language Explanations** (GPT-4 powered or Generate Manually)\n",
    "- **Banking-Specific Features** (37 features, 13 categories, 19 fraud patterns)\n",
    "- **BSA/AML Compliance** (Velocity checks)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6054b66",
   "metadata": {},
   "source": [
    "Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a838fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45073f9",
   "metadata": {},
   "source": [
    "Step 2: Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import ssl\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# API imports\n",
    "from openai import AzureOpenAI\n",
    "# Load environment variables (force override)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Configuration\n",
    "ANOMALY_THRESHOLD = float(os.getenv('ANOMALY_THRESHOLD', '0.70'))  # Banking-optimized\n",
    "USE_AZURE = os.getenv('USE_AZURE', 'false').lower() == 'true'\n",
    "USE_LLaMA = os.getenv('USE_LLaMA', 'false').lower() == 'true'\n",
    "LLAMA_API_KEY = os.getenv('LLAMA_API_KEY', '')\n",
    "LLAMA_API_BASE_URL = os.getenv('LLAMA_API_BASE_URL', 'https://api.llama.com/v1/')\n",
    "\n",
    "print(f\"Anomaly Threshold: {ANOMALY_THRESHOLD}\")\n",
    "print(f\"Using Azure OpenAI: {USE_AZURE}\")\n",
    "print(f\"Using LLaMA API: {USE_LLaMA}\")\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398b2b4",
   "metadata": {},
   "source": [
    "Step 3: Loading banking transactions dataset\n",
    "\n",
    "**Dataset Split Strategy (80/20):**\n",
    "- **Training**: 80% of legitimate transactions (~3,300) - teaches model what \"normal\" looks like\n",
    "- **Testing**: 20% of legitimate (~826) + ALL fraudulent (874) = ~1,700 total\n",
    "- **Coverage**: Uses 100% of the 5,000 transaction dataset (no waste!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35078cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from JSON file\n",
    "print(\"Loading banking transactions dataset...\")\n",
    "\n",
    "with open('data/banking_transactions.json', 'r') as f:\n",
    "    all_transactions = json.load(f)\n",
    "\n",
    "# Split into training (legitimate only) and test sets\n",
    "# Use 80/20 split on ALL data (uses 100% of dataset)\n",
    "legitimate_txns = [t for t in all_transactions if not t['is_fraud']]\n",
    "fraudulent_txns = [t for t in all_transactions if t['is_fraud']]\n",
    "\n",
    "# 80% of legitimate transactions for training\n",
    "train_size_legit = int(0.8 * len(legitimate_txns))\n",
    "\n",
    "# Training: 80% of legitimate transactions only (anomaly detection learns from normal data)\n",
    "training_data = legitimate_txns[:train_size_legit]\n",
    "\n",
    "# Testing: remaining 20% of legitimate + ALL fraudulent transactions\n",
    "test_transactions = legitimate_txns[train_size_legit:] + fraudulent_txns\n",
    "\n",
    "print(f\"Loaded dataset:\")\n",
    "print(f\"   - Total transactions: {len(all_transactions):,}\")\n",
    "print(f\"   - Legitimate: {len(legitimate_txns):,} | Fraudulent: {len(fraudulent_txns):,}\")\n",
    "print(f\"\\nData Split (80/20):\")\n",
    "print(f\"   - Training set: {len(training_data):,} transactions (legitimate only)\")\n",
    "print(f\"   - Test set: {len(test_transactions):,} transactions\")\n",
    "print(f\"     - Legitimate: {len(legitimate_txns[train_size_legit:]):,}\")\n",
    "print(f\"     - Fraudulent: {len(fraudulent_txns):,}\")\n",
    "print(f\"   - Test fraud rate: {len(fraudulent_txns)/len(test_transactions):.1%}\")\n",
    "print(f\"\\nUsing 100% of dataset (no data wasted!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b771b92",
   "metadata": {},
   "source": [
    "Step 4: Banking Data Processor\n",
    "Handles:\n",
    "- 13 merchant categories (ATM, Wire, Gambling, etc.)\n",
    "- 19 high-risk patterns\n",
    "- Transaction type classification\n",
    "- Velocity checks (BSA/AML compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BankingDataProcessor:\n",
    "    \"\"\"Process banking transactions with industry-specific features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 13 Banking merchant categories\n",
    "        self.banking_categories = {\n",
    "            'atm': ['atm', 'cash withdrawal', 'cashpoint'],\n",
    "            'wire_transfer': ['wire', 'international transfer', 'remittance', 'swift'],\n",
    "            'bill_payment': ['utility', 'bill pay', 'payment service'],\n",
    "            'gambling': ['casino', 'gambling', 'betting', 'lottery', 'poker'],\n",
    "            'cash_advance': ['cash advance', 'payday', 'quick cash'],\n",
    "            'investment': ['brokerage', 'investment', 'trading', 'securities'],\n",
    "            'p2p': ['venmo', 'paypal', 'zelle', 'cash app', 'peer to peer'],\n",
    "            'check': ['check cashing', 'check deposit', 'mobile deposit'],\n",
    "            'loan': ['loan payment', 'mortgage', 'credit line'],\n",
    "            'insurance': ['insurance', 'premium payment'],\n",
    "            'government': ['irs', 'tax payment', 'government fee'],\n",
    "            'crypto': ['crypto', 'bitcoin', 'coinbase', 'blockchain'],\n",
    "            'retail': ['store', 'shop', 'market', 'purchase']\n",
    "        }\n",
    "        \n",
    "        # 19 High-risk patterns\n",
    "        self.high_risk_patterns = [\n",
    "            'wire', 'transfer', 'international', 'offshore', 'crypto', 'bitcoin',\n",
    "            'gambling', 'casino', 'betting', 'cash advance', 'payday',\n",
    "            'gift card', 'prepaid', 'money order', 'western union',\n",
    "            'check cashing', 'pawn', 'atm', 'withdrawal'\n",
    "        ]\n",
    "        \n",
    "        # Transaction types\n",
    "        self.transaction_types = {\n",
    "            'debit': ['purchase', 'payment', 'withdrawal', 'debit'],\n",
    "            'credit': ['refund', 'credit', 'deposit', 'return'],\n",
    "            'transfer': ['transfer', 'wire', 'ach'],\n",
    "            'withdrawal': ['withdrawal', 'atm', 'cash'],\n",
    "            'deposit': ['deposit', 'mobile deposit'],\n",
    "            'payment': ['bill', 'payment', 'utility']\n",
    "        }\n",
    "        \n",
    "        # Velocity thresholds (BSA/AML)\n",
    "        self.velocity_thresholds = {\n",
    "            'hourly_count': 10,\n",
    "            'daily_count': 50,\n",
    "            'hourly_amount': 10000,\n",
    "            'daily_amount': 50000\n",
    "        }\n",
    "        \n",
    "        self.user_histories = defaultdict(list)\n",
    "    \n",
    "    def extract_transaction_type(self, description: str) -> str:\n",
    "        \"\"\"Classify transaction type\"\"\"\n",
    "        desc_lower = description.lower()\n",
    "        for txn_type, keywords in self.transaction_types.items():\n",
    "            if any(keyword in desc_lower for keyword in keywords):\n",
    "                return txn_type\n",
    "        return 'other'\n",
    "    \n",
    "    def check_velocity(self, user_id: str, timestamp: datetime, amount: float) -> Dict[str, Any]:\n",
    "        \"\"\"Check velocity limits for BSA/AML compliance\"\"\"\n",
    "        user_txns = self.user_histories[user_id]\n",
    "        \n",
    "        # Count transactions in last hour and day\n",
    "        one_hour_ago = timestamp - timedelta(hours=1)\n",
    "        one_day_ago = timestamp - timedelta(days=1)\n",
    "        \n",
    "        hourly_txns = [t for t in user_txns if t['timestamp'] >= one_hour_ago]\n",
    "        daily_txns = [t for t in user_txns if t['timestamp'] >= one_day_ago]\n",
    "        \n",
    "        hourly_count = len(hourly_txns)\n",
    "        daily_count = len(daily_txns)\n",
    "        hourly_amount = sum(t['amount'] for t in hourly_txns)\n",
    "        daily_amount = sum(t['amount'] for t in daily_txns)\n",
    "        \n",
    "        return {\n",
    "            'hourly_count': hourly_count,\n",
    "            'daily_count': daily_count,\n",
    "            'hourly_amount': hourly_amount,\n",
    "            'daily_amount': daily_amount,\n",
    "            'count_breach_hourly': hourly_count > self.velocity_thresholds['hourly_count'],\n",
    "            'count_breach_daily': daily_count > self.velocity_thresholds['daily_count'],\n",
    "            'amount_breach_hourly': hourly_amount > self.velocity_thresholds['hourly_amount'],\n",
    "            'amount_breach_daily': daily_amount > self.velocity_thresholds['daily_amount']\n",
    "        }\n",
    "    \n",
    "    def extract_banking_features(self, transaction: Dict[str, Any]) -> Dict[str, bool]:\n",
    "        \"\"\"Extract 12 banking-specific boolean features\"\"\"\n",
    "        desc_lower = transaction['description'].lower()\n",
    "        merchant_lower = transaction.get('merchant', '').lower()\n",
    "        \n",
    "        return {\n",
    "            'is_atm': any(k in desc_lower or k in merchant_lower for k in self.banking_categories['atm']),\n",
    "            'is_wire': any(k in desc_lower or k in merchant_lower for k in self.banking_categories['wire_transfer']),\n",
    "            'is_gambling': any(k in desc_lower or k in merchant_lower for k in self.banking_categories['gambling']),\n",
    "            'is_cash_advance': any(k in desc_lower or k in merchant_lower for k in self.banking_categories['cash_advance']),\n",
    "            'is_p2p': any(k in desc_lower or k in merchant_lower for k in self.banking_categories['p2p']),\n",
    "            'is_crypto': any(k in desc_lower or k in merchant_lower for k in self.banking_categories['crypto']),\n",
    "            'is_check': any(k in desc_lower or k in merchant_lower for k in self.banking_categories['check']),\n",
    "            'is_card_present': transaction.get('card_present', False),\n",
    "            'is_card_not_present': not transaction.get('card_present', True),\n",
    "            'is_cross_border': transaction.get('location', '').lower() in ['international', 'foreign', 'offshore'],\n",
    "            'is_mobile': 'mobile' in desc_lower or 'app' in desc_lower,\n",
    "            'has_high_risk_pattern': any(pattern in desc_lower or pattern in merchant_lower \n",
    "                                         for pattern in self.high_risk_patterns)\n",
    "        }\n",
    "    \n",
    "    def process_transaction(self, transaction: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single banking transaction\"\"\"\n",
    "        user_id = transaction['user_id']\n",
    "        timestamp = datetime.fromisoformat(transaction['timestamp'])\n",
    "        amount = transaction['amount']\n",
    "        \n",
    "        # Extract features\n",
    "        txn_type = self.extract_transaction_type(transaction['description'])\n",
    "        velocity = self.check_velocity(user_id, timestamp, amount)\n",
    "        banking_features = self.extract_banking_features(transaction)\n",
    "        \n",
    "        # Categorize merchant\n",
    "        merchant_lower = transaction.get('merchant', '').lower()\n",
    "        desc_lower = transaction['description'].lower()\n",
    "        category = 'other'\n",
    "        for cat, keywords in self.banking_categories.items():\n",
    "            if any(k in merchant_lower or k in desc_lower for k in keywords):\n",
    "                category = cat\n",
    "                break\n",
    "        \n",
    "        # Build processed transaction\n",
    "        processed = {\n",
    "            **transaction,\n",
    "            'transaction_type': txn_type,\n",
    "            'merchant_category': category,\n",
    "            'velocity': velocity,\n",
    "            'banking_features': banking_features,\n",
    "            'timestamp_dt': timestamp\n",
    "        }\n",
    "        \n",
    "        # Update history\n",
    "        self.user_histories[user_id].append({\n",
    "            'timestamp': timestamp,\n",
    "            'amount': amount\n",
    "        })\n",
    "        \n",
    "        return processed\n",
    "\n",
    "print(\"Banking Data Processor ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad948d2c",
   "metadata": {},
   "source": [
    "Step 5: Anomaly Detection System\n",
    "Uses 4 ML models:\n",
    "1. Isolation Forest\n",
    "2. Local Outlier Factor\n",
    "3. One-Class SVM\n",
    "4. Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAnomalyDetector:\n",
    "    \"\"\"Multi-model anomaly detection with embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'models/all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize detector with local embedding model\n",
    "        Args:\n",
    "            model_name: Path to local model folder (default: 'models/all-MiniLM-L6-v2')\n",
    "        \"\"\"\n",
    "        print(f\"Loading embedding model from: {model_name}\")\n",
    "        try:\n",
    "            # Try to load from local folder first\n",
    "            self.embedding_model = SentenceTransformer(model_name, device='cpu')\n",
    "            print(f\"Loaded model from local folder: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load from {model_name}: {e}\")\n",
    "            print(\"Attempting to download 'all-MiniLM-L6-v2' from HuggingFace...\")\n",
    "            try:\n",
    "                # Fallback: download from HuggingFace (with SSL disabled)\n",
    "                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "                print(\"Model downloaded and loaded successfully\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Failed to download model: {e2}\")\n",
    "                raise RuntimeError(\"Could not load embedding model. Please check 'models' folder or internet connection.\")\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=0.95)\n",
    "        \n",
    "        # 4 ML models\n",
    "        # The Mahalanobis Distance model is different from the other three. \n",
    "        # It's not a scikit-learn class hence we dont need to initialize. \n",
    "        self.isolation_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "        self.lof = LocalOutlierFactor(contamination=0.1, novelty=True, n_neighbors=20)\n",
    "        self.one_class_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.1)\n",
    "        \n",
    "        self.is_trained = False\n",
    "        self.user_histories = defaultdict(list)\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings from text descriptions\"\"\"\n",
    "        return self.embedding_model.encode(texts, show_progress_bar=False)\n",
    "    \n",
    "    def extract_numerical_features(self, transaction: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Extract 37 numerical features for banking\"\"\"\n",
    "        user_id = transaction['user_id']\n",
    "        amount = transaction['amount']\n",
    "        timestamp = transaction['timestamp_dt']\n",
    "        \n",
    "        # User history stats\n",
    "        user_amounts = self.user_histories.get(user_id, [amount])\n",
    "        mean_amount = np.mean(user_amounts)\n",
    "        median_amount = np.median(user_amounts)\n",
    "        max_amount = np.max(user_amounts)\n",
    "        p95_amount = np.percentile(user_amounts, 95)\n",
    "        \n",
    "        # 37 Features\n",
    "        features = [\n",
    "            # Amount features (9)\n",
    "            amount,\n",
    "            np.log1p(amount),\n",
    "            1 if amount % 100 == 0 else 0,  # Round number\n",
    "            1 if amount > 1000 else 0,\n",
    "            1 if amount > 5000 else 0,\n",
    "            amount / (mean_amount + 1),\n",
    "            amount / (median_amount + 1),\n",
    "            amount / (max_amount + 1),\n",
    "            1 if amount > p95_amount else 0,\n",
    "            \n",
    "            # Temporal features (7)\n",
    "            timestamp.hour,\n",
    "            timestamp.weekday(),\n",
    "            timestamp.day,\n",
    "            1 if timestamp.weekday() >= 5 else 0,  # Weekend\n",
    "            1 if timestamp.hour < 6 or timestamp.hour >= 22 else 0,  # Night\n",
    "            1 if 9 <= timestamp.hour <= 17 else 0,  # Business hours\n",
    "            timestamp.timestamp(),\n",
    "            \n",
    "            # Location features (3)\n",
    "            1 if transaction.get('location', '').lower() in ['online', 'internet'] else 0,\n",
    "            1 if transaction.get('location', '').lower() == 'international' else 0,\n",
    "            1 if transaction['banking_features']['is_cross_border'] else 0,\n",
    "            \n",
    "            # Banking-specific features (10)\n",
    "            1 if transaction['banking_features']['is_atm'] else 0,\n",
    "            1 if transaction['banking_features']['is_gambling'] else 0,\n",
    "            1 if transaction['banking_features']['is_cash_advance'] else 0,\n",
    "            1 if transaction['banking_features']['is_wire'] else 0,\n",
    "            1 if transaction['banking_features']['is_p2p'] else 0,\n",
    "            1 if transaction['banking_features']['is_card_present'] else 0,\n",
    "            1 if transaction['banking_features']['is_card_not_present'] else 0,\n",
    "            1 if transaction['banking_features']['is_check'] else 0,\n",
    "            1 if transaction['banking_features']['is_mobile'] else 0,\n",
    "            1 if transaction['banking_features']['has_high_risk_pattern'] else 0,\n",
    "            \n",
    "            # Velocity features (8)\n",
    "            transaction['velocity']['hourly_count'],\n",
    "            transaction['velocity']['daily_count'],\n",
    "            transaction['velocity']['hourly_amount'],\n",
    "            transaction['velocity']['daily_amount'],\n",
    "            1 if transaction['velocity']['count_breach_hourly'] else 0,\n",
    "            1 if transaction['velocity']['count_breach_daily'] else 0,\n",
    "            1 if transaction['velocity']['amount_breach_hourly'] else 0,\n",
    "            1 if transaction['velocity']['amount_breach_daily'] else 0,\n",
    "        ]\n",
    "        \n",
    "        return np.array(features, dtype=float)\n",
    "    \n",
    "    def train(self, transactions: List[Dict[str, Any]]):\n",
    "        \"\"\"Train all 4 models\"\"\"\n",
    "        print(f\"Training on {len(transactions)} transactions...\")\n",
    "        \n",
    "        # Build user histories\n",
    "        for txn in transactions:\n",
    "            user_id = txn['user_id']\n",
    "            self.user_histories[user_id].append(txn['amount'])\n",
    "        \n",
    "        # Generate embeddings\n",
    "        texts = [f\"{t['description']} {t.get('merchant', '')}\" for t in transactions]\n",
    "        embeddings = self.generate_embeddings(texts)\n",
    "        \n",
    "        # Extract numerical features\n",
    "        numerical_features = np.array([self.extract_numerical_features(t) for t in transactions])\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = np.hstack([embeddings, numerical_features])\n",
    "        \n",
    "        # Scale and reduce dimensions\n",
    "        scaled_features = self.scaler.fit_transform(combined_features)\n",
    "        reduced_features = self.pca.fit_transform(scaled_features)\n",
    "        \n",
    "        # Train models\n",
    "        self.isolation_forest.fit(reduced_features)\n",
    "        self.lof.fit(reduced_features)\n",
    "        self.one_class_svm.fit(reduced_features)\n",
    "        \n",
    "        # Store for Mahalanobis\n",
    "        #self.mean = The center point (mean) of all legitimate transactions\n",
    "        #self.cov_inv = The inverse covariance matrix (how features vary together)\n",
    "        self.mean = np.mean(reduced_features, axis=0)\n",
    "        self.cov_inv = np.linalg.pinv(np.cov(reduced_features.T))\n",
    "        \n",
    "        self.is_trained = True\n",
    "        print(\"All models trained!\")\n",
    "    \n",
    "    def predict_single(self, transaction: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"Predict anomaly score for single transaction\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Models not trained yet!\")\n",
    "        \n",
    "        # Generate features\n",
    "        text = f\"{transaction['description']} {transaction.get('merchant', '')}\"\n",
    "        embedding = self.generate_embeddings([text])[0]\n",
    "        numerical = self.extract_numerical_features(transaction)\n",
    "        \n",
    "        # Combine and transform\n",
    "        combined = np.hstack([embedding, numerical]).reshape(1, -1)\n",
    "        scaled = self.scaler.transform(combined)\n",
    "        reduced = self.pca.transform(scaled)\n",
    "        \n",
    "        # Get scores from all 4 models\n",
    "        if_score = -self.isolation_forest.score_samples(reduced)[0]\n",
    "        lof_score = -self.lof.score_samples(reduced)[0]\n",
    "        svm_score = -self.one_class_svm.score_samples(reduced)[0]\n",
    "        \n",
    "        # Mahalanobis distance\n",
    "        diff = reduced[0] - self.mean\n",
    "        mahal_dist = np.sqrt(diff @ self.cov_inv @ diff.T)\n",
    "        \n",
    "        # Normalize scores to 0-1\n",
    "        scores = {\n",
    "            'isolation_forest': min(max(if_score / 2 + 0.5, 0), 1),\n",
    "            'local_outlier_factor': min(max(lof_score / 2 + 0.5, 0), 1),\n",
    "            'one_class_svm': min(max(svm_score / 2 + 0.5, 0), 1),\n",
    "            'mahalanobis': min(max(mahal_dist / 10, 0), 1)\n",
    "        }\n",
    "        \n",
    "        # Ensemble score (average)\n",
    "        ensemble_score = np.mean(list(scores.values()))\n",
    "        \n",
    "        return ensemble_score, scores\n",
    "\n",
    "print(\"Anomaly Detector ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafad869",
   "metadata": {},
   "source": [
    "Step 6: Natural Language Explanation Generator\n",
    "\n",
    "Uses GPT-4 to generate human-readable fraud explanations or Generate explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationGenerator:\n",
    "    \"\"\"Generate natural language explanations for fraud alerts\"\"\"\n",
    "    \n",
    "    def __init__(self, use_azure: bool = True, use_llama: bool = False):\n",
    "        \"\"\"Initialize explanation generator\n",
    "        \n",
    "        Args:\n",
    "            use_azure: If True, use custom Azure OpenAI with OAuth2 authentication\n",
    "            use_llama: If True, use LLaMA API for explanations\n",
    "            If both False, use template-based explanations\n",
    "        \"\"\"\n",
    "        self.use_azure = use_azure\n",
    "        self.use_llama = use_llama\n",
    "        \n",
    "        if use_azure:\n",
    "            print(\"Using custom Azure OpenAI with OAuth2 authentication\")\n",
    "        elif use_llama:\n",
    "            print(\"Using LLaMA API for explanations\")\n",
    "        else:\n",
    "            print(\"Using template-based explanations\")\n",
    "    \n",
    "    def generate_explanation(self, transaction: Dict[str, Any], anomaly_score: float, \n",
    "                           model_scores: Dict[str, float]) -> str:\n",
    "        \"\"\"Generate fraud explanation\"\"\"\n",
    "        \n",
    "        # Build context\n",
    "        risk_factors = []\n",
    "        if transaction['amount'] > 5000:\n",
    "            risk_factors.append(f\"Large amount (${transaction['amount']:,.2f})\")\n",
    "        if transaction['timestamp_dt'].hour < 6 or transaction['timestamp_dt'].hour >= 22:\n",
    "            risk_factors.append(\"Night-time transaction\")\n",
    "        if transaction['banking_features']['is_wire']:\n",
    "            risk_factors.append(\"Wire transfer\")\n",
    "        if transaction['banking_features']['is_gambling']:\n",
    "            risk_factors.append(\"Gambling merchant\")\n",
    "        if transaction['banking_features']['is_cross_border']:\n",
    "            risk_factors.append(\"International/Cross-border\")\n",
    "        if transaction['velocity']['count_breach_hourly']:\n",
    "            risk_factors.append(\"Velocity breach (hourly)\")\n",
    "        \n",
    "        # Try Azure OpenAI if enabled\n",
    "        if self.use_azure:\n",
    "            try:\n",
    "                from azure_openai import azure_chat\n",
    "                import asyncio\n",
    "                \n",
    "                prompt = f\"\"\"You are a banking fraud analyst. Explain why this transaction was flagged as suspicious.\n",
    "\n",
    "Transaction Details:\n",
    "- Description: {transaction['description']}\n",
    "- Merchant: {transaction.get('merchant', 'N/A')}\n",
    "- Amount: ${transaction['amount']:,.2f}\n",
    "- Category: {transaction['merchant_category']}\n",
    "- Type: {transaction['transaction_type']}\n",
    "- Time: {transaction['timestamp_dt'].strftime('%Y-%m-%d %I:%M %p')}\n",
    "- Location: {transaction.get('location', 'N/A')}\n",
    "\n",
    "Risk Score: {anomaly_score:.2%}\n",
    "Risk Factors: {', '.join(risk_factors) if risk_factors else 'None specific'}\n",
    "Model Scores: {', '.join(f'{k}: {v:.2f}' for k, v in model_scores.items())}\n",
    "\n",
    "Provide a clear, concise explanation (2-3 sentences) and recommend specific actions.\"\"\"\n",
    "                \n",
    "                # Call custom Azure OpenAI function (async)\n",
    "                try:\n",
    "                    # Try to get the current event loop\n",
    "                    loop = asyncio.get_event_loop()\n",
    "                    if loop.is_running():\n",
    "                        # We're in Jupyter - use nest_asyncio\n",
    "                        import nest_asyncio\n",
    "                        nest_asyncio.apply()\n",
    "                        response_content = asyncio.run(azure_chat(prompt))\n",
    "                    else:\n",
    "                        response_content = asyncio.run(azure_chat(prompt))\n",
    "                except:\n",
    "                    # Fallback: create new event loop\n",
    "                    response_content = asyncio.run(azure_chat(prompt))\n",
    "                \n",
    "                return response_content\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Azure OpenAI call failed (falling back to template): {e}\")\n",
    "        \n",
    "        # Try LLaMA API if enabled\n",
    "        elif self.use_llama:\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "                \n",
    "                # Validate API credentials\n",
    "                if not LLAMA_API_KEY or LLAMA_API_KEY.strip() == '':\n",
    "                    raise ValueError(\"LLAMA_API_KEY is not configured. Please set it in .env file\")\n",
    "                \n",
    "                # Initialize LLaMA API client\n",
    "                client = OpenAI(\n",
    "                    api_key=LLAMA_API_KEY,\n",
    "                    base_url=LLAMA_API_BASE_URL,\n",
    "                )\n",
    "                \n",
    "                prompt = f\"\"\"You are a banking fraud analyst. Explain why this transaction was flagged as suspicious.\n",
    "\n",
    "Transaction Details:\n",
    "- Description: {transaction['description']}\n",
    "- Merchant: {transaction.get('merchant', 'N/A')}\n",
    "- Amount: ${transaction['amount']:,.2f}\n",
    "- Category: {transaction['merchant_category']}\n",
    "- Type: {transaction['transaction_type']}\n",
    "- Time: {transaction['timestamp_dt'].strftime('%Y-%m-%d %I:%M %p')}\n",
    "- Location: {transaction.get('location', 'N/A')}\n",
    "\n",
    "Risk Score: {anomaly_score:.2%}\n",
    "Risk Factors: {', '.join(risk_factors) if risk_factors else 'None specific'}\n",
    "Model Scores: {', '.join(f'{k}: {v:.2f}' for k, v in model_scores.items())}\n",
    "\n",
    "Provide a clear, concise explanation (2-3 sentences) and recommend specific actions.\"\"\"\n",
    "                \n",
    "                # Call LLaMA API with proper error handling\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=[\n",
    "                    {\"role\": \"assistant\", \"content\": prompt},\n",
    "                    ],\n",
    "                    model=\"Llama-3.3-8B-Instruct\",\n",
    "                    temperature=0.6,\n",
    "                    max_completion_tokens=2048,\n",
    "                    top_p=0.9,\n",
    "                    frequency_penalty=1\n",
    "                )\n",
    "                # Check if response is valid and has the expected structure\n",
    "                # Validate response\n",
    "                if response is None:\n",
    "                    raise ValueError(\"LLaMA API returned None response\")\n",
    "\n",
    "                # LLaMA returns completion_message instead of choices\n",
    "                if hasattr(response, \"completion_message\") and response.completion_message:\n",
    "                    content_block = response.completion_message.get(\"content\")\n",
    "\n",
    "                    if isinstance(content_block, dict) and content_block.get(\"type\") == \"text\":\n",
    "                        content = content_block.get(\"text\")\n",
    "\n",
    "                        if not content or content.strip() == \"\":\n",
    "                            raise ValueError(\"LLaMA API returned empty content\")\n",
    "\n",
    "                        return content\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError(\"Unexpected LLaMA content format\")\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"LLaMA API response missing completion_message\")\n",
    "\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"LLaMA API call failed (falling back to template): {e}\")\n",
    "        \n",
    "        # Fallback template-based explanation\n",
    "        explanation = f\"\"\"**Fraud Alert**\n",
    "\n",
    "This transaction scored {anomaly_score:.1%} on our fraud detection models.\n",
    "\n",
    "**Risk Factors Detected:**\n",
    "{chr(10).join(f'â€¢ {factor}' for factor in risk_factors) if risk_factors else 'â€¢ Multiple anomalous patterns detected'}\n",
    "\n",
    "**Model Analysis:**\n",
    "â€¢ Isolation Forest: {model_scores['isolation_forest']:.2f}\n",
    "â€¢ Local Outlier Factor: {model_scores['local_outlier_factor']:.2f}\n",
    "â€¢ One-Class SVM: {model_scores['one_class_svm']:.2f}\n",
    "â€¢ Mahalanobis Distance: {model_scores['mahalanobis']:.2f}\n",
    "\n",
    "**Recommended Actions:**\n",
    "â€¢ Contact account holder immediately for verification\n",
    "â€¢ Review recent account activity for similar patterns\n",
    "â€¢ Consider temporary hold pending confirmation\n",
    "â€¢ Document for compliance review (SAR/CTR if applicable)\n",
    "\"\"\"\n",
    "        return explanation\n",
    "\n",
    "print(\"Explanation Generator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507105d1",
   "metadata": {},
   "source": [
    "Step 7: Complete Fraud Detection System\n",
    "\n",
    "Orchestrates all components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f780206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BankingFraudDetectionSystem:\n",
    "    \"\"\"Complete banking fraud detection system\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.70, use_azure: bool = True, use_llama: bool = False):\n",
    "        self.processor = BankingDataProcessor()\n",
    "        self.detector = EmbeddingAnomalyDetector()\n",
    "        self.explainer = ExplanationGenerator(use_azure=use_azure, use_llama=use_llama)\n",
    "        self.threshold = threshold\n",
    "        self.results = []\n",
    "    \n",
    "    def train(self, transactions: List[Dict[str, Any]]):\n",
    "        \"\"\"Train the system\"\"\"\n",
    "        print(f\"\\nTraining Banking Fraud Detection System...\")\n",
    "        processed_txns = [self.processor.process_transaction(t) for t in transactions]\n",
    "        self.detector.train(processed_txns)\n",
    "        print(f\"System trained on {len(transactions)} transactions\\n\")\n",
    "    \n",
    "    def analyze_transaction(self, transaction: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze single transaction\"\"\"\n",
    "        # Process\n",
    "        processed = self.processor.process_transaction(transaction)\n",
    "        \n",
    "        # Detect\n",
    "        anomaly_score, model_scores = self.detector.predict_single(processed)\n",
    "        \n",
    "        # Determine if fraud\n",
    "        is_fraud = anomaly_score >= self.threshold\n",
    "        \n",
    "        # Generate explanation if fraud\n",
    "        explanation = None\n",
    "        if is_fraud:\n",
    "            explanation = self.explainer.generate_explanation(processed, anomaly_score, model_scores)\n",
    "        \n",
    "        # Categorize risk\n",
    "        if anomaly_score >= 0.9:\n",
    "            risk_level = \"CRITICAL\"\n",
    "        elif anomaly_score >= 0.75:\n",
    "            risk_level = \"HIGH\"\n",
    "        elif anomaly_score >= 0.5:\n",
    "            risk_level = \"MEDIUM\"\n",
    "        else:\n",
    "            risk_level = \"LOW\"\n",
    "        \n",
    "        result = {\n",
    "            'transaction': transaction,\n",
    "            'processed': processed,\n",
    "            'is_fraud': is_fraud,\n",
    "            'anomaly_score': anomaly_score,\n",
    "            'model_scores': model_scores,\n",
    "            'risk_level': risk_level,\n",
    "            'explanation': explanation\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get fraud detection summary\"\"\"\n",
    "        if not self.results:\n",
    "            return {}\n",
    "        \n",
    "        total = len(self.results)\n",
    "        fraud_count = sum(1 for r in self.results if r['is_fraud'])\n",
    "        \n",
    "        return {\n",
    "            'total_transactions': total,\n",
    "            'fraud_detected': fraud_count,\n",
    "            'fraud_rate': fraud_count / total if total > 0 else 0,\n",
    "            'avg_score': np.mean([r['anomaly_score'] for r in self.results]),\n",
    "            'risk_distribution': {\n",
    "                'CRITICAL': sum(1 for r in self.results if r['risk_level'] == 'CRITICAL'),\n",
    "                'HIGH': sum(1 for r in self.results if r['risk_level'] == 'HIGH'),\n",
    "                'MEDIUM': sum(1 for r in self.results if r['risk_level'] == 'MEDIUM'),\n",
    "                'LOW': sum(1 for r in self.results if r['risk_level'] == 'LOW')\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"Complete Fraud Detection System ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315a818",
   "metadata": {},
   "source": [
    "Step 8: Train the System\n",
    "\n",
    "Train the fraud detection system on 80% of legitimate transactions (~3,300) from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a9a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system\n",
    "fraud_system = BankingFraudDetectionSystem(\n",
    "    threshold=ANOMALY_THRESHOLD,\n",
    "    use_azure=USE_AZURE,\n",
    "    use_llama=USE_LLaMA\n",
    ")\n",
    "\n",
    "# Train on normal transactions\n",
    "fraud_system.train(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e236da6",
   "metadata": {},
   "source": [
    "Step 9: Analyze Test Transactions\n",
    "\n",
    "Run fraud detection on test transactions and compare with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adb657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Analyze test transactions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYZING TEST TRANSACTIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Process all transactions\n",
    "print(f\"Processing {len(test_transactions)} transactions...\\n\")\n",
    "for txn in test_transactions:\n",
    "    fraud_system.analyze_transaction(txn)\n",
    "\n",
    "# Show only FRAUD cases (reduces output by ~50%)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FRAUD ALERTS DETECTED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fraud_results = [r for r in fraud_system.results if r['is_fraud']]\n",
    "\n",
    "if fraud_results:\n",
    "    print(f\"\\nFound {len(fraud_results)} fraudulent transactions:\\n\")\n",
    "    \n",
    "    for result in fraud_results:\n",
    "        txn = result['transaction']\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Transaction: {txn['transaction_id']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Description: {txn['description']}\")\n",
    "        print(f\"Merchant: {txn['merchant']}\")\n",
    "        print(f\"Amount: ${txn['amount']:,.2f}\")\n",
    "        print(f\"Time: {datetime.fromisoformat(txn['timestamp']).strftime('%Y-%m-%d %I:%M %p')}\")\n",
    "        print(f\"Location: {txn['location']}\")\n",
    "        print(f\"\\nFRAUD DETECTION RESULT:\")\n",
    "        print(f\"   Risk Score: {result['anomaly_score']:.1%}\")\n",
    "        print(f\"   Risk Level: {result['risk_level']}\")\n",
    "        print(f\"   Fraud Status: FRAUD DETECTED\")\n",
    "        print(f\"\\nModel Scores:\")\n",
    "        for model, score in result['model_scores'].items():\n",
    "            print(f\"   â€¢ {model.replace('_', ' ').title()}: {score:.2f}\")\n",
    "        \n",
    "        if result['explanation']:\n",
    "            print(f\"\\nEXPLANATION:\\n\")\n",
    "            print(result['explanation'])\n",
    "        \n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"\\nNo fraud detected in test set!\")\n",
    "\n",
    "print(f\"\\nTip: See full analysis (including legitimate transactions) in the summary below.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4d8be",
   "metadata": {},
   "source": [
    "Step 10: Model Performance & Summary\n",
    "\n",
    "Evaluate model performance with ground truth labels and view statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Get summary\n",
    "summary = fraud_system.get_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FRAUD DETECTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Transactions Analyzed: {summary['total_transactions']}\")\n",
    "print(f\"Fraud Detected: {summary['fraud_detected']}\")\n",
    "print(f\"Fraud Rate: {summary['fraud_rate']:.1%}\")\n",
    "print(f\"Average Risk Score: {summary['avg_score']:.1%}\")\n",
    "\n",
    "print(f\"\\nRisk Level Distribution:\")\n",
    "for level, count in summary['risk_distribution'].items():\n",
    "    print(f\"   {level}: {count} transactions\")\n",
    "\n",
    "# Model Performance Evaluation (compare with ground truth)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_true = [r['transaction']['is_fraud'] for r in fraud_system.results]\n",
    "y_pred = [r['is_fraud'] for r in fraud_system.results]\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"   True Negatives (Correct Legit):  {tn:4d}\")\n",
    "print(f\"   False Positives (False Alarms):  {fp:4d}\")\n",
    "print(f\"   False Negatives (Missed Fraud):  {fn:4d}\")\n",
    "print(f\"   True Positives (Caught Fraud):   {tp:4d}\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy:.2%}\")\n",
    "print(f\"   Precision: {precision:.2%} (of flagged, how many are actually fraud)\")\n",
    "print(f\"   Recall:    {recall:.2%} (of actual fraud, how many we caught)\")\n",
    "print(f\"   F1 Score:  {f1:.2%} (balanced precision-recall)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "results_df = pd.DataFrame([{\n",
    "    'Transaction ID': r['transaction']['transaction_id'],\n",
    "    'Merchant': r['transaction']['merchant'][:25],\n",
    "    'Amount': f\"${r['transaction']['amount']:,.2f}\",\n",
    "    'Risk Score': f\"{r['anomaly_score']:.1%}\",\n",
    "    'Predicted': 'FRAUD' if r['is_fraud'] else 'OK',\n",
    "    'Actual': 'FRAUD' if r['transaction']['is_fraud'] else 'OK',\n",
    "    'Match': 'YES' if r['is_fraud'] == r['transaction']['is_fraud'] else 'NO'\n",
    "} for r in fraud_system.results[:100]])  # Show first 20\n",
    "\n",
    "print(\"\\nSAMPLE RESULTS (First 100):\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n... and more\\n\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b577d13a",
   "metadata": {},
   "source": [
    "Manual Testing - Test Your Own Transactions\n",
    "\n",
    "Use this cell to manually test any transaction and see fraud detection in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de16c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transaction(description, merchant, amount, location=\"Online\", hour=14):\n",
    "    \"\"\"\n",
    "    Quick function to test a transaction manually\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    description : str\n",
    "        Transaction description (e.g., \"Bitcoin purchase\")\n",
    "    merchant : str\n",
    "        Merchant name (e.g., \"Crypto.com\")\n",
    "    amount : float\n",
    "        Transaction amount (e.g., 8500.00)\n",
    "    location : str, optional\n",
    "        Location (e.g., \"Online\", \"International\", \"New York\")\n",
    "    hour : int, optional\n",
    "        Hour of day 0-23 (default: 14 = 2 PM)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create timestamp\n",
    "    now = datetime.now()\n",
    "    test_time = now.replace(hour=hour, minute=0, second=0)\n",
    "    \n",
    "    # Build transaction\n",
    "    test_txn = {\n",
    "        'transaction_id': 'TEST001',\n",
    "        'user_id': 'test_user',\n",
    "        'amount': amount,\n",
    "        'merchant': merchant,\n",
    "        'description': description,\n",
    "        'timestamp': test_time.isoformat(),\n",
    "        'location': location,\n",
    "        'card_present': False,\n",
    "        'is_fraud': False  # We're testing, so this is unknown\n",
    "    }\n",
    "    \n",
    "    # Analyze\n",
    "    print(\"=\"*80)\n",
    "    print(\"TESTING YOUR TRANSACTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTransaction Details:\")\n",
    "    print(f\"   Description: {description}\")\n",
    "    print(f\"   Merchant: {merchant}\")\n",
    "    print(f\"   Amount: ${amount:,.2f}\")\n",
    "    print(f\"   Location: {location}\")\n",
    "    print(f\"   Time: {test_time.strftime('%I:%M %p')}\")\n",
    "    \n",
    "    # Run fraud detection\n",
    "    result = fraud_system.analyze_transaction(test_txn)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FRAUD DETECTION RESULT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n   Risk Score: {result['anomaly_score']:.1%}\")\n",
    "    print(f\"   Risk Level: {result['risk_level']}\")\n",
    "    \n",
    "    if result['is_fraud']:\n",
    "        print(f\"   Status: FRAUD DETECTED\")\n",
    "    else:\n",
    "        print(f\"   Status: LEGITIMATE\")\n",
    "    \n",
    "    print(f\"\\nModel Scores:\")\n",
    "    for model_name, score in result['model_scores'].items():\n",
    "        print(f\"   - {model_name.replace('_', ' ').title()}: {score:.2f}\")\n",
    "    \n",
    "    if result['explanation']:\n",
    "        print(f\"\\nEXPLANATION:\")\n",
    "        print(\"=\"*80)\n",
    "        print(result['explanation'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Manual testing function loaded!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  test_transaction('Bitcoin purchase', 'Crypto.com', 8500, 'International', 2)\")\n",
    "print(\"  test_transaction('Grocery shopping', 'Walmart', 127.50, 'New York', 14)\")\n",
    "print(\"  test_transaction('Wire transfer', 'Bank', 15000, 'International', 23)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248ce91",
   "metadata": {},
   "source": [
    "Create Your Own Test - Edit and Run!\n",
    "\n",
    "**Instructions:**\n",
    "1. Change the values below to test your own transaction\n",
    "2. Run the cell to see fraud detection result\n",
    "3. Try different combinations to understand the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08eb999",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Test Fraud Detection with Azure Explanations or Manual Template explanation\n",
    "\n",
    "Now test a fraud transaction to see Azure OpenAI-generated explanation (make sure USE_AZURE=true in .env) or Template Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fraud detection with LLM-generated explanation\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING FRAUD DETECTION WITH LLM EXPLANATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# High-risk transaction that should trigger fraud detection\n",
    "result = test_transaction(\n",
    "    description=\"Wire transfer to offshore account Bitcoin purchase\",\n",
    "    merchant=\"International Wire Service\", \n",
    "    amount=12500.00,\n",
    "    location=\"International\",\n",
    "    hour=2  # 2 AM\n",
    ")\n",
    "\n",
    "# Check which backend was used for explanation\n",
    "print(\"\\nLLM Backend Status:\")\n",
    "print(f\"   Configuration:\")\n",
    "print(f\"   - USE_AZURE: {USE_AZURE}\")\n",
    "print(f\"   - USE_LLaMA: {USE_LLaMA}\")\n",
    "\n",
    "if result['explanation']:\n",
    "    # Determine which backend was likely used based on explanation format\n",
    "    is_template = \"**Fraud Alert**\" in result['explanation']\n",
    "    \n",
    "    if USE_AZURE:\n",
    "        if is_template:\n",
    "            print(\"\\n   Status: Azure OpenAI was configured but call failed\")\n",
    "            print(\"   Fallback: Using template-based explanation\")\n",
    "        else:\n",
    "            print(\"\\n   Status: Successfully used Azure OpenAI (GPT-4)\")\n",
    "            print(\"   âœ“ Natural language explanation generated\")\n",
    "    elif USE_LLaMA:\n",
    "        if is_template:\n",
    "            print(\"\\n   Status: LLaMA API was configured but call failed\")\n",
    "            print(\"   Fallback: Using template-based explanation\")\n",
    "        else:\n",
    "            print(\"\\n   Status: Successfully used LLaMA API\")\n",
    "            print(\"   âœ“ Natural language explanation generated\")\n",
    "    else:\n",
    "        print(\"\\n   Status: No LLM backend enabled\")\n",
    "        print(\"   Using: Template-based explanation\")\n",
    "        print(\"   To enable LLMs:\")\n",
    "        print(\"      - Set USE_AZURE=true for Azure OpenAI\")\n",
    "        print(\"      - Set USE_LLaMA=true for LLaMA API\")\n",
    "else:\n",
    "    print(\"\\n   Status: No explanation generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa52920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CUSTOM TEST - Edit these values and run!\n",
    "test_transaction(\n",
    "    description=\"Buy Bitcoin\",  # e.g., \"Coffee purchase\", \"Bitcoin trade\"\n",
    "    merchant=\"Coinbase\",        # e.g., \"Starbucks\", \"Coinbase\"\n",
    "    amount=100.00,                        # e.g., 50.00, 10000.00\n",
    "    location=\"Online\",                    # e.g., \"New York\", \"International\", \"Online\"\n",
    "    hour=2                               # 0-23 (e.g., 2=2am, 14=2pm, 23=11pm)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
